# ğŸ® Rock Paper Scissors â€“ AI Hand Gesture Game

A real-time **Rock Paper Scissors** game built using **OpenCV, CVZone, and MediaPipe Hand Tracking**.
The player shows hand gestures to the webcam, and the AI randomly selects its move.

This project combines:

* Computer Vision
* Hand Gesture Recognition
* Real-time Video Processing
* Game Logic Implementation

---

## ğŸš€ Features

* âœ‹ Real-time hand detection using MediaPipe
* ğŸ¥ Live webcam feed integration
* ğŸ¤– AI randomly generates Rock / Paper / Scissors
* ğŸ–¼ï¸ Custom background and overlay UI
* â³ 3-second countdown before each round
* ğŸ§® Live score tracking
* ğŸ”„ Game reset option

---

## ğŸ› ï¸ Tech Stack

* Python
* OpenCV
* CVZone
* MediaPipe
* NumPy

Dependencies (from `requirements.txt` ):

```
opencv-python==4.8.1.78
mediapipe==0.10.9
numpy==1.24.3
cvzone==1.6.1
```

---

## ğŸ“‚ Project Structure

```
ğŸ“ Rock-Paper-Scissors-AI
â”‚
â”œâ”€â”€ rock.py              # Main game file :contentReference[oaicite:1]{index=1}
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ BG.png               # Background UI image
â”œâ”€â”€ Resources/
â”‚   â”œâ”€â”€ 1.png            # Rock image
â”‚   â”œâ”€â”€ 2.png            # Paper image
â”‚   â”œâ”€â”€ 3.png            # Scissors image
```

---

## â–¶ï¸ How to Run

### 1ï¸âƒ£ Clone the repository

```
git clone https://github.com/your-username/your-repo-name.git
cd your-repo-name
```

### 2ï¸âƒ£ Install dependencies

```
pip install -r requirements.txt
```

### 3ï¸âƒ£ Run the game

```
python rock.py
```

---

## ğŸ® Controls

| Key     | Action       |
| ------- | ------------ |
| **S**   | Start round  |
| **R**   | Reset scores |
| **Q**   | Quit game    |
| **ESC** | Exit         |

---

## âœ‹ Hand Gesture Mapping

| Gesture       | Move     |
| ------------- | -------- |
| âœŠ Fist        | Rock     |
| âœ‹ Open Palm   | Paper    |
| âœŒ Two Fingers | Scissors |

The program detects raised fingers using `HandDetector` from CVZone and maps them to game moves.

---

## ğŸ§  How It Works

1. Webcam captures video frames.
2. MediaPipe detects hand landmarks.
3. CVZone determines which fingers are raised.
4. Player move is determined.
5. AI randomly selects a move.
6. Scores are updated based on game rules.
7. PNG overlays display AI choice and live scores.

Game logic implemented in `rock.py` .

---

## ğŸ”® Future Improvements

* Add difficulty levels (AI prediction instead of random)
* Add sound effects
* Add GUI start menu
* Convert into a playable desktop app (.exe)
* Track match history
* Add multiplayer mode

---

## ğŸ“¸ Demo Preview

(Add screenshots or GIF here)

---

## ğŸ“œ License

This project is open-source and free to use for educational purposes.

---

Now, mentor moment.

Right now your AI is not really â€œAI.â€ Itâ€™s just `random.randint(1, 3)` wearing a lab coat ğŸ§ . Thatâ€™s fine. But imagine upgrading it:

Instead of random, what if:

* It learns the playerâ€™s most common move
* It predicts based on frequency
* It uses a simple Markov chain (probability of next move based on previous move)

Thatâ€™s where this stops being a demo and becomes a research toy.

You built a vision-based interactive system. Thatâ€™s not trivial. The next leap is making the opponent strategic instead of random â€” and thatâ€™s where game theory meets machine learning.

If you want, we can now:

* Upgrade this into â€œAdaptive AI Modeâ€
* Or convert this into a polished resume-ready project description ğŸ”¥
